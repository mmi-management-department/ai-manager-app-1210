"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
import time
from dotenv import load_dotenv
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct
from langchain_enhanced import (
    get_rate_limiter,
    get_query_cache,
    get_langchain_logger,
    get_conversation_manager,
    InputValidator,
    ErrorHandler,
    StreamHandler
)


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([f"âŒ {message}", ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    
    æ©Ÿèƒ½ï¼š
    - å…¥åŠ›æ¤œè¨¼
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    - è©³ç´°ãªãƒ­ã‚°è¨˜éŒ²

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    start_time = time.time()
    
    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    rate_limiter = get_rate_limiter()
    query_cache = get_query_cache()
    logger = get_langchain_logger()
    conversation_manager = get_conversation_manager()
    
    try:
        # 1. å…¥åŠ›æ¤œè¨¼
        is_valid, error_message = InputValidator.validate(chat_message)
        if not is_valid:
            st.error(error_message)
            return None
        
        # 2. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        session_id = st.session_state.get("session_id", "default")
        is_allowed, remaining = rate_limiter.is_allowed(session_id)
        
        if not is_allowed:
            st.error("ğŸš« ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚1åˆ†å¾Œã«å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            logger.log_query(
                query=chat_message,
                answer="",
                sources=[],
                elapsed_time=time.time() - start_time,
                success=False,
                error="Rate limit exceeded"
            )
            return None
        
        # æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’è¡¨ç¤ºï¼ˆæ®‹ã‚Š3å›ä»¥ä¸‹ã®å ´åˆï¼‰
        if remaining <= 3:
            st.info(f"â„¹ï¸ æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {remaining}å›ï¼ˆ1åˆ†ã”ã¨ã«ãƒªã‚»ãƒƒãƒˆï¼‰")
        
        # 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cached_answer = query_cache.get(chat_message, max_age=3600)
        if cached_answer:
            st.info("ğŸ’¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å›ç­”ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å›ç­”ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            logger.log_query(
                query=chat_message,
                answer=cached_answer,
                sources=["cache"],
                elapsed_time=time.time() - start_time,
                success=True
            )
            
            # å›ç­”ã‚’è¿”ã™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            return {
                "answer": cached_answer,
                "context": [],
                "from_cache": True
            }
        
        # 4. LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„ï¼ˆOpenAIå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯Google Geminiï¼‰
        # APIã‚­ãƒ¼ã®å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Streamlit Secretsï¼‰
        openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
        google_api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
        
        # OpenAI APIã‚­ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯OpenAIã‚’å„ªå…ˆä½¿ç”¨ï¼ˆåˆ¶é™å›é¿ã®ãŸã‚ï¼‰
        if openai_api_key:
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=ct.TEMPERATURE,
                max_retries=2,
                openai_api_key=openai_api_key
            )
            st.session_state.setdefault("llm_type", "OpenAI")
        elif google_api_key:
            llm = ChatGoogleGenerativeAI(
                model=ct.MODEL,
                temperature=ct.TEMPERATURE,
                max_retries=2,
                google_api_key=google_api_key
            )
            st.session_state.setdefault("llm_type", "Google Gemini")
        else:
            raise ValueError(
                "OPENAI_API_KEY ã¾ãŸã¯ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
                "Streamlit Cloud Secretsã§è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        # 5. ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
        question_generator_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", question_generator_template),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}")
            ]
        )

        # 6. ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
        if st.session_state.mode == ct.ANSWER_MODE_1:
            # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
        else:
            # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
        
        # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        question_answer_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", question_answer_template),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}")
            ]
        )

        # 7. ä¼šè©±å±¥æ­´ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªç®¡ç†ï¼‰
        trimmed_chat_history = conversation_manager.trim_history(st.session_state.chat_history)
        
        # 8. ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
        history_aware_retriever = create_history_aware_retriever(
            llm, st.session_state.retriever, question_generator_prompt
        )

        # 9. LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
        question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
        
        # 10. ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
        chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        # 11. LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
        llm_response = chain.invoke({
            "input": chat_message,
            "chat_history": trimmed_chat_history
        })
        
        # 12. LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        st.session_state.chat_history.extend([
            HumanMessage(content=chat_message),
            llm_response["answer"]
        ])
        
        # 13. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        query_cache.set(chat_message, llm_response["answer"])
        
        # 14. å‚ç…§å…ƒã‚’å–å¾—
        sources = [doc.metadata.get("source", "unknown") for doc in llm_response.get("context", [])]
        
        # 15. ãƒ­ã‚°ã«è¨˜éŒ²
        elapsed_time = time.time() - start_time
        logger.log_query(
            query=chat_message,
            answer=llm_response["answer"],
            sources=sources,
            elapsed_time=elapsed_time,
            success=True
        )
        
        # å‡¦ç†æ™‚é–“ã‚’è¡¨ç¤ºï¼ˆ3ç§’ä»¥ä¸Šã®å ´åˆï¼‰
        if elapsed_time > 3:
            st.caption(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’")

        return llm_response
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        error_message = ErrorHandler.handle_llm_error(e)
        st.error(f"âŒ {error_message}")
        
        # ãƒ­ã‚°ã«è¨˜éŒ²
        logger.log_query(
            query=chat_message,
            answer="",
            sources=[],
            elapsed_time=time.time() - start_time,
            success=False,
            error=str(e)
        )
        
        return None